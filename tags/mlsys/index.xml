<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>mlsys on Mingjie&#39;s Home</title>
    <link>https://caaatch22.github.io/tags/mlsys/</link>
    <description>Recent content in mlsys on Mingjie&#39;s Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 20 Feb 2024 15:35:59 +0000</lastBuildDate><atom:link href="https://caaatch22.github.io/tags/mlsys/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TinyMl —— pruning</title>
      <link>https://caaatch22.github.io/posts/tinyml-pruning/</link>
      <pubDate>Tue, 20 Feb 2024 15:35:59 +0000</pubDate>
      
      <guid>https://caaatch22.github.io/posts/tinyml-pruning/</guid>
      <description>最近正在学习 MIT 6.5940, 韩松老师的课，做deep learning compression的应该都只知道。课程分为三个部分，efficient inference, domain-specific optimization, efficient training。有完整的课件，视频和实验。最后一个lab是将llama2部署在个人电脑上，非常有意思（谁不想要个自己的大模型呢）。其余lab也都可以白嫖google colab的gpu
Introduction 正式介绍pruning and sparsity之前，我们先来聊聊为什么要做model compression这个事情。 Today&amp;rsquo;s Model is Too Big!
随着Large language model的出现，如GPT-3，如今的模型参数量已经达到了上百billion，别说训练，我们甚至无法在一个gpu上对其进行推理。更别提如果我们想要将其部署在其他边缘设备上。
所以当前在做inference之前，一般都会有个model-compression的过程，包括pruning（剪枝），quantization（量化），distillation（蒸馏）等。这些方法都是为了减少模型的大小，加速推理过程。这些方法也被广泛地集成到了各种加速卡，gpu中。例如nv的A100就支持structured sparsity（[N:M]形式的，具体含义下文会详细介绍）。
Efficiency Metrics 我们再来看看一些 efficiency metrics，这也是我们在做inference过程中需要考虑的指标：
Memory-Related Metrics # parameters model size total/peak #activations Computation-Related Metrics MACs FLOP, FLOPs # parameters 下表是一些常见结构的参数数量：
Model #Parameters Linear Layer(FC) $feature_{in} * feature_{out}$ Conv Layer $c_{i} * c_{o} * k_{h} * k_{w} $ Grouped Conv Layer $c_{i} * c_{o} * k_{h} * k_{w} / g$ Depthwise Conv Layer $c_{o} * k_{h} * k_{w}$ 其中，Grouped Conv指的是将输入在channel维度进行分组，然后分别进行卷积，最后concatenate。Depthwise Conv是分组个数 $g$ 等于输入channel数的情况。</description>
      <content:encoded><![CDATA[<blockquote>
<p>最近正在学习 <a href="https://hanlab.mit.edu/courses/2023-fall-65940">MIT 6.5940</a>, <a href="https://hanlab.mit.edu/songhan">韩松</a>老师的课，做deep learning compression的应该都只知道。课程分为三个部分，<strong>efficient inference, domain-specific optimization, efficient training</strong>。有完整的课件，视频和实验。最后一个lab是将llama2部署在个人电脑上，非常有意思（谁不想要个自己的大模型呢）。其余lab也都可以白嫖google colab的gpu</p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>正式介绍pruning and sparsity之前，我们先来聊聊为什么要做model compression这个事情。
<img loading="lazy" src="/img/tinyml/todays-model-size.png" alt="todays-model-size"  />
</p>
<p><strong>Today&rsquo;s Model is Too Big!</strong></p>
<p>随着Large language model的出现，如GPT-3，如今的模型参数量已经达到了上百billion，别说训练，我们甚至无法在一个gpu上对其进行推理。更别提如果我们想要将其部署在其他边缘设备上。</p>
<p>所以当前在做inference之前，一般都会有个model-compression的过程，包括pruning（剪枝），quantization（量化），distillation（蒸馏）等。这些方法都是为了减少模型的大小，加速推理过程。这些方法也被广泛地集成到了各种加速卡，gpu中。例如nv的A100就支持structured sparsity（[N:M]形式的，具体含义下文会详细介绍）。</p>
<h3 id="efficiency-metrics">Efficiency Metrics</h3>
<p>我们再来看看一些 efficiency metrics，这也是我们在做inference过程中需要考虑的指标：</p>
<ul>
<li>Memory-Related Metrics
<ul>
<li># parameters</li>
<li>model size</li>
</ul>
</li>
<li>total/peak #activations</li>
<li>Computation-Related Metrics
<ul>
<li>MACs</li>
<li>FLOP, FLOPs</li>
</ul>
</li>
</ul>
<h4 id="-parameters"># parameters</h4>
<p>下表是一些常见结构的参数数量：</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>#Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Layer(FC)</td>
<td>$feature_{in} * feature_{out}$</td>
</tr>
<tr>
<td>Conv Layer</td>
<td>$c_{i} * c_{o} * k_{h} * k_{w} $</td>
</tr>
<tr>
<td>Grouped Conv Layer</td>
<td>$c_{i} * c_{o} * k_{h} * k_{w} / g$</td>
</tr>
<tr>
<td>Depthwise Conv Layer</td>
<td>$c_{o} * k_{h} * k_{w}$</td>
</tr>
</tbody>
</table>
<p>其中，Grouped Conv指的是将输入在channel维度进行分组，然后分别进行卷积，最后concatenate。Depthwise Conv是分组个数 $g$ 等于输入channel数的情况。</p>
<p>除了这些weight外，还有bias以及norm相关的参数。
例如，对于batchnorm层，我们需要两个参数$\gamma,  \beta$以及runing mean和running variance。
$$
y = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$</p>
<p>这都是针对infernece的情况，对于training，还需要考虑一些额外的参数，例如momentum，以及保存梯度等等。</p>
<h4 id="model-size">model size</h4>
<p>$$
Model Size = \# parameters * bitwidth
$$</p>
<p>举个例子，AlexNet有$61M$个参数，如果我们用32bit的float来表示，那么model size就是$61M * 4byte = 224MB$。但如果我们用8-bit来表示每个weight，那么model size就是$61M * 1byte = 61MB$。</p>
<p>这就是quantization的一个应用，通过减少bitwidth来减少model size。</p>
<h4 id="totalpeak-activations">total/peak #activations</h4>
<p>#activations 是模型在推理时在内存中需要存储的中间结果，这也可能成为内存瓶颈。如下图所示：
<img loading="lazy" src="/img/tinyml/mcunet-activations.png" alt="activation"  />
</p>
<p>该图展示了MCUNet(一个专门用在IoT设备上的模型)的参数量和activations数量对比resnet的减少。可以看出，参数的减少量十分显著，但是# activations不降反增，和param的占比是一个数量级的。所以我们若想在边缘设备上部署模型，需要考虑activations的数量。</p>
<h4 id="mac">MAC</h4>
<p><code>MAC</code>的含义是 <strong>Multiply-Accumulate operation</strong>。例如，一个gemm(genearl matrix multiplication)的$MACs = m * n * k$（对于m * k的矩阵和k * n的矩阵相乘）。深度学习中几乎90%的时间都在做gemm，一个conv2d操作也可以由im2col转换为gemm操作。</p>
<p>以下是常见的一些layer的MACs计算：</p>
<table>
<thead>
<tr>
<th>layer</th>
<th>MACs(batch size = 1)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear</td>
<td>$feature_{in} * feature_{out}$</td>
</tr>
<tr>
<td>Conv</td>
<td>$c_{i}  * k_{h} * k_{w} * h_{o} * w_{o} * c_{o}$</td>
</tr>
<tr>
<td>grouped conv</td>
<td>$c_{i}/g * k_{h} * k_{w} * h_{o} * w_{o}* c_{o} $</td>
</tr>
<tr>
<td>depthwise conv</td>
<td>$c_{o} * k_{h} * k_{w} * h_{o} * w_{o}$</td>
</tr>
</tbody>
</table>
<h4 id="flop-and-flops">FLOP and FLOPs</h4>
<p>FLOP的意思是 <strong>Floating Point Operations</strong>，是指浮点运算的次数。FLOPs是指每秒的FLOP数量。
一般一个MAC对应两个FLOP，一乘一加。</p>
<h3 id="energy-consumption">Energy Consumption</h3>
<p>对于边缘设备，我们还需要考虑能耗的问题。
<img loading="lazy" src="/img/tinyml/memory-is-expensive.png" alt="energy-consumption"  />

图中可以看出，从内存中取数操作的能耗是运算的200倍以上。</p>
<h2 id="neural-network-pruning">neural network pruning</h2>
<p>那么，到底什么是 <em>模型剪枝（model pruning）</em> 呢？顾名思义，就是将模型中的一些参数去掉，或者更加细粒度的，我们可以把某些参数与参数之间的连接去掉。还有一个常用的概念是 <em>稀疏度（sparsity）</em> ，指的是剪枝后的参数占原有参数的比例。例如，如果我们剪去了90%的参数，那么sparsity就是0.1。</p>
<p><img loading="lazy" src="/img/tinyml/nnpruning.png" alt="pruning"  />
</p>
<blockquote>
<p><strong>注意：剪去某个参数并不意味着单纯将其设置为0。<strong>因为我们剪枝的目的是</strong>减少内存使用</strong>以及<strong>加速推理</strong>，若只是讲某个$W$矩阵中的一些值变成0，并不能达到减少内存的目的。我们需要将剪枝后的参数$W_{p}$用特定的方式存储（例如，稀疏矩阵）以及结合特定的优化后的运算才能减少内存使用并加速。当然，从正确性的角度来说，将某些参数设置为0可以得到一样的结果。所以我们可以在确定 <em>sparsity</em> 时先将一定的参数置为0，然后再通过fine-tuning来确定剪去这么多的参数是否会影响模型最终的效果。</p>
</blockquote>
<p>减去一定的参数后，肯定会对我们的模型效果造成影响。这时候我们需要进行fine-tuning调整参数分布，具体会在后文展开。</p>
<p>形式化的定义prune：
<img loading="lazy" src="/img/tinyml/prune-formalized.png" alt="prune-formalized"  />
</p>
<h3 id="pruning-granularity">pruning granularity</h3>
<p>我们有不同的剪枝粒度，from unstructured to structured。</p>
<p>对于一个FC layer，其实就是将一个2d matrix进行剪枝：
<img loading="lazy" src="/img/tinyml/fc-prune.png" alt="fc-prune"  />

如图，细粒度的、不规则的剪枝一般能有更高的sparsity，但是却难以加速（就像上文说的，如果只是将一些参数设置为0，不会有任何内存与速度上的收益）。而粗粒度的剪枝，就可以不需要改变原有的矩阵结构获得更少的内存使用以及加速，但一般sparsity会高一些（同等accuracy下）。</p>
<p>对于卷积层，我们有更多的剪枝粒度：
<img loading="lazy" src="/img/tinyml/conv-prune.png" alt="conv-prune"  />

也是从完全不规则，到有一定的pattern，到vector-level，kernel-level再到剪去一个通道。优缺点如同上面分析的一般。
对于最fine-grained的剪枝，许多模型都可以达到剪去90%以上而不影响精度。
<img loading="lazy" src="/img/tinyml/fine-grained-prune-sparsity.png" alt="fine-grained-prune-sparsity"  />
</p>
<p>对于有一定pattern的剪枝，就需要设计特定的存储方式或者运算方式来加速。
例如下图的[N:M]剪枝（每M个参数剪去N个）就可以通过只存非0权重，以及一个下标数组的方式减少内存占用。nvdia的Ampere系列GPU就在其Tensor core内融合了这种优化，达到了2倍的加速效果
<img loading="lazy" src="/img/tinyml/pattern-grain-prune.png" alt="pattern-grain-prune"  />
</p>
<p>对于conv的剪枝，我们还用的一个不需要硬件特定实现的方法就是直接进行<code>channel-level pruning</code>，也就是直接减去特定的通道。</p>
<ul>
<li>pros: 直接的加速，无需特定硬件</li>
<li>cons: 更少的模型压缩比
不同的卷积层的sparsity又该如何确定呢？是所有卷积层的sparsity都设置的相同还是各不相同？有什么样的标准呢？我们会在在pruning ratio这里会讲到。</li>
</ul>
<h3 id="pruning-criterion">pruning criterion</h3>
<p>如何确定剪去的权重呢？常见的有<code>magnitude-based pruning</code>，<code>scaling-based pruning</code>等</p>
<h4 id="magnitude-based-pruning">Magnitude-based pruning</h4>
<blockquote>
<p>Learning Structured Sparsity in Deep Neural Networks <a href="https://arxiv.org/pdf/1608.03665.pdf">Wen et al., NeurIPS 2016</a></p>
</blockquote>
<p>只得就是通过权重的重要程度来进行剪枝。直觉上看，对于一个激活函数$y = ReLU(10x_0 - 8x_1 + 0.1x_2 )$，肯定是$0.1$这个权重对其结果影响最小，可以将其剪去。
依此定义重要度$Importance = |W|$，直接按照数值的大小进行剪枝。当然，我们也可以将其拓展到任意范数：
$$
Importance = |W|<em>p = (\sum</em>{i} |W_i|^p)^{\frac{1}{p}}
$$
只不过最常用的还是l1范数，因为这引入的额外的计算量较小。</p>
<h4 id="scaling-based-pruning">Scaling-based pruning</h4>
<blockquote>
<p>Learning Efficient Convolutional Networks through Network Slimming <a href="https://arxiv.org/pdf/1708.06519.pdf">Liu et al., ICCV 2017</a></p>
</blockquote>
<p>scaling是针对卷积的一种剪枝策略，我们对卷积操作后的每个通道都用一个 <em>scaling factor</em> 进行缩放。这个scaling factor就和其他参数一样是一个需要训练的参数。
<img loading="lazy" src="/img/tinyml/scaling-based-pruning.png" alt="scaling-prune"  />

我们在以scaling factor的大小代表这一channel的重要性，从而图中一样进行剪枝。事实上，我们并不需要额外的scaling factor，因为我们的batchnorm层有自带了一个scaling factor: $\gamma$ ($z = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$)。我们可以直接用这个$\gamma$来进行剪枝。</p>
<h4 id="second-order-based-pruning">Second-Order-based-pruning</h4>
<blockquote>
<p>Optimal Brain Damage <a href="https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf">LeCun et al., NeurIPS 1989</a></p>
</blockquote>
<p>前面两种剪枝标准都是启发式的，通过定义参数的重要性进行剪枝。我们还可以通过 <em>最小化剪枝前后的损失函数</em> 来进行剪枝。
$$
\delta L = L(\mathbf{x}; \mathbf{W}) - L(\mathbf{x}; \mathbf{W}<em>P = \mathbf{W} - \delta\mathbf{W})  = \sum</em>{i} g_{i}\delta w_{i} + \frac{1}{2}\sum_{i} h_{ii}\delta w_{i}^2 + \frac{1}{2}\sum_{i \neq j} h_{ij}\delta w_{i}\delta w_{j} + O(|\delta\mathbf{W}|^3) \ where\ \ g_i = \frac{\partial L}{\partial w_i}, h_{ij} = \frac{\partial^2 L}{\partial w_i \partial w_j}
$$
这是LeCun在1989年时候的工作，它假设：</p>
<ul>
<li>目标函数L是近似二次的，所以最后一项可忽略</li>
<li>这个神经网络的训练已经收敛了，这表明一阶导数（梯度）接近零，所以第一项 $(\sum_{i}g_{i}\delta w_{i})$ 也可忽略</li>
<li>删除每个参数引起的误差是独立的：这允许我们忽略混合二阶导数项 ，因为这些项描述的是参数之间的相互作用。</li>
</ul>
<p>因此，我们可以得到：
$$
\delta L_i = L(x; W) - L(x; W_p| w_i = 0) \approx \frac{1}{2}h_{ii} w_i^2
$$
也就是$importance_{w_i} = |\delta L| = \frac{1}{2}h_{ii}w_{i}^2$，这就是一个二阶导数的剪枝标准。但是，由于$h_{ii}$ 是Hessian matrix，计算量过大，所以一般不会使用。</p>
<h4 id="percentage-of-zero-based-pruning">percentage-of-Zero-Based pruning</h4>
<blockquote>
<p>Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures <a href="https://arxiv.org/pdf/1607.03250.pdf">Hu et al.,ArXiv 2017</a></p>
</blockquote>
<p>非常简单粗暴的对与Channel-level pruning的方法。由于Relu会产生0，我们直接定义每个通道的重要程度为其 <em>Average Percentage of Zeros(APoZ)</em>。选取最小APoZ的通道进行剪枝。</p>
<h4 id="regression-based-pruning">Regression-based pruning</h4>
<blockquote>
<p>Channel Pruning for Accelerating Very Deep Neural Networks <a href="https://arxiv.org/pdf/1707.06168.pdf">He et al., ICCV 2017</a></p>
</blockquote>
<p>regression-based pruning对于某一个layer进行操作，希望能够最小化重构误差。
令输入的feature map的channel数为$c$, 卷积核$W$的权重为 $n× c × k_h×k_w$, 卷积核每次卷积会在一个像素点上生成一个$N×n$的输出矩阵$Y$,其中$N$为batch_num，这里暂时不考虑bias项。要将$c$修剪为$c′$.同时最小化reconstruction error，这个优化问题是
$$
\begin{aligned}
&amp; \underset{\beta, W}{\text{arg min}}
&amp; &amp; \frac{1}{2N} | Y - \sum_{i=1}^{C} \beta_i X_i W_i^T |_F^2 \
&amp; \text{subject to}
&amp; &amp; | \beta |_0 \leq C'
\end{aligned}
$$</p>
<p>其中$X_{i}$是一个$N \times k_h k_w$的矩阵裁剪自输入$X$。求解这个问题是NP难的,这里首先将问题用l1范数松弛为</p>
<p>$$
\begin{aligned}
&amp; \underset{\beta, W}{\text{arg min}}
&amp; &amp; \frac{1}{2N} \left| Y - \sum_{i=1}^{C} \beta_i X_i W_i^T \right|_F^2 + \lambda \left| \beta \right|_1 \
&amp; \text{subject to}
&amp; &amp; \left| \beta \right|_0 \leq C&rsquo;, ; \forall i, \left| W_i \right|_F = 1
\end{aligned}
$$
同时限制$||Wi||F=1$，然后在以下两个步骤中迭代</p>
<ul>
<li>首先锁定$W$ ,求解$\beta$, 作为channel selection问题，这变成了零范数的LASSO regression。代码中可以知道作者是使用sklearn的Lasso regression函数做的</li>
<li>锁定$\beta$，问题变成了$\underset{W&rsquo;}{\text{arg min}} \left| Y - X&rsquo;(W&rsquo;)^T \right|_F^2
$本质上是一个线性回归。</li>
</ul>
<h3 id="pruning-ratio">Pruning Ratio</h3>
<p>对于不同的layer之间，怎么确定每个layer应该剪去多少的权重而不会过多的影响精度呢？</p>
<h4 id="sensitivity-of-each-layer">sensitivity of each layer</h4>
<p>一种直观的方法就是查看每个layer对权重的敏感度各是多少，具体操作如下：</p>
<ul>
<li>选定一个layer $L_i$
<ul>
<li>prune $li$ with ratio $r_i \in {0.1, 0.2, &hellip;, 0.9}$(or other strides)</li>
<li>对每个ratio观察精度下降$\Delta Acc_r^i$</li>
</ul>
</li>
<li>对所有layer重复上述操作</li>
</ul>
<p><img loading="lazy" src="/img/tinyml/prune-ratio.png" alt="prune-ratio"  />

图中是VGG-11在CIFAR-10数据集上的不同层的pruning ratio对精度的影响。选定一个threshold后就可以依据这个对不同layer选取不同的pruning ratio。</p>
<h4 id="automatic-pruning">automatic pruning</h4>
<p>上面的方法是一种手动的方法，并且没有考虑不同层之间的相互影响。
一个有意思的工作来自ECCV 2018, <a href="https://arxiv.org/pdf/1802.03494.pdf">AMC: AutoML for Model Compression and Acceleration on Mobile Devices</a>, 用了强化学习的方法来选取不同的pruning-ratio，效果很好。不过目前没看懂，后面有机会再补充。</p>
<h3 id="fine-tuningtraining">Fine-tuning/Training</h3>
<p>说了这么多pruing方法，具体的操作流程如下：
<img loading="lazy" src="/img/tinyml/iterative-prune.png" alt="iterative-prune"  />
</p>
<p>通常我们都会迭代式的再inference阶段进行pruning，例如，先采用50%的sparsity进行推理，得到精度误差后fine-tune一次，训练出新的weights分布，然后不断加大sparsity，不断重新fine-tune。如上图所示</p>
<h2 id="system--hardware-support-for-sparsity">System &amp; Hardware Support for Sparsity</h2>
<p>在pruning granularity的时候我们说到，若想要做到细粒度的剪枝，就需要特定的硬件支持。
韩老师自己的一篇论文[EIE](<a href="https://arxiv.org/pdf/1602.01528.pdfEIE">https://arxiv.org/pdf/1602.01528.pdfEIE</a>: Efficient Inference Engine on Compressed Deep Neural Network)就支持对任意粒度的剪枝进行加速。由于我不是做硬件的，暂时没有仔细研究实现。</p>
<p>英伟达的Amper系列GPU也支持对于structured sparsity的加速，例如[N:M]剪枝。
架构图如下：
<img loading="lazy" src="/img/tinyml/m-n-sparsity.png" alt="m-n-sparsity"  />
</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
