<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>deep-learning on Mingjie&#39;s Home</title>
    <link>https://caaatch22.github.io/tags/deep-learning/</link>
    <description>Recent content in deep-learning on Mingjie&#39;s Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 20 Feb 2024 15:35:59 +0000</lastBuildDate><atom:link href="https://caaatch22.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>tinyml —— pruning</title>
      <link>https://caaatch22.github.io/posts/tinyml-pruning/</link>
      <pubDate>Tue, 20 Feb 2024 15:35:59 +0000</pubDate>
      
      <guid>https://caaatch22.github.io/posts/tinyml-pruning/</guid>
      <description> 最近正在学习 MIT 6.5940, 韩松老师的课，做deep learning compression的应该都只知道。课程分为三个部分，efficient inference, domain-specific optimization, efficient training。有完整的课件，视频和实验。最后一个lab是将llama2部署在个人电脑上，非常有意思（谁不想要个自己的大模型呢）。其余lab也都可以白嫖google colab的gpu
Introduction 正式介绍pruning and sparsity之前，我们先来聊聊为什么要做model compression这个事情。 Today&amp;rsquo;s Model is Too Big!
随着Large language model的出现，如GPT-3，如今的模型参数量已经达到了上百billion，别说训练，我们甚至无法在一个gpu上对其进行推理。更别提如果我们想要将其部署在其他边缘设备上。
所以当前在做inference之前，一般都会有个model-compression的过程，包括pruning（剪枝），quantization（量化），distillation（蒸馏）等。这些方法都是为了减少模型的大小，加速推理过程。这些方法也被广泛地集成到了各种加速卡，gpu中。例如nv的A100就支持structured sparsity（[N:M]形式的，具体含义下文会详细介绍）。
我们再来看看一些 efficiency metrics，主要包括两类：
Memory-Related Metrics # parameters model size Computation-Related Metrics FLOP, FLOPs MACs Latency, throughput 下表是一些常见的模型结构的参数量
Model #Parameters Linear Layer(FC) $feature_{in} * feature_{out}$ Conv Layer $c_{i} * c_{o} * k_{h} * k_{w} $ Group Conv Layer $c_{i} * c_{o} * k_{h} * k_{w} / g$ Depthwise Conv Layer $c_{o} * k_{h} * k_{w}$ notations $n$ batch size $c_{i}/c_{o}$ Input/Output Channels $k_{h}/k_{w}$ kernel height/weight $g$ group number </description>
      <content:encoded><![CDATA[<blockquote>
<p>最近正在学习 <a href="https://hanlab.mit.edu/courses/2023-fall-65940">MIT 6.5940</a>, <a href="https://hanlab.mit.edu/songhan">韩松</a>老师的课，做deep learning compression的应该都只知道。课程分为三个部分，<strong>efficient inference, domain-specific optimization, efficient training</strong>。有完整的课件，视频和实验。最后一个lab是将llama2部署在个人电脑上，非常有意思（谁不想要个自己的大模型呢）。其余lab也都可以白嫖google colab的gpu</p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>正式介绍pruning and sparsity之前，我们先来聊聊为什么要做model compression这个事情。
<img loading="lazy" src="/img/tinyml/todays-model-size.png" alt="todays-model-size"  />
</p>
<p><strong>Today&rsquo;s Model is Too Big!</strong></p>
<p>随着Large language model的出现，如GPT-3，如今的模型参数量已经达到了上百billion，别说训练，我们甚至无法在一个gpu上对其进行推理。更别提如果我们想要将其部署在其他边缘设备上。</p>
<p>所以当前在做inference之前，一般都会有个model-compression的过程，包括pruning（剪枝），quantization（量化），distillation（蒸馏）等。这些方法都是为了减少模型的大小，加速推理过程。这些方法也被广泛地集成到了各种加速卡，gpu中。例如nv的A100就支持structured sparsity（[N:M]形式的，具体含义下文会详细介绍）。</p>
<p>我们再来看看一些 efficiency metrics，主要包括两类：</p>
<ul>
<li>Memory-Related Metrics
<ul>
<li># parameters</li>
<li>model size</li>
</ul>
</li>
<li>Computation-Related Metrics
<ul>
<li>FLOP, FLOPs</li>
<li>MACs</li>
<li>Latency, throughput</li>
</ul>
</li>
</ul>
<p>下表是一些常见的模型结构的参数量</p>
<!-- raw HTML omitted -->
<table>
<thead>
<tr>
<th>Model</th>
<th>#Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Layer(FC)</td>
<td>$feature_{in} * feature_{out}$</td>
</tr>
<tr>
<td>Conv Layer</td>
<td>$c_{i} * c_{o} * k_{h} * k_{w} $</td>
</tr>
<tr>
<td>Group Conv Layer</td>
<td>$c_{i} * c_{o} * k_{h} * k_{w} / g$</td>
</tr>
<tr>
<td>Depthwise Conv Layer</td>
<td>$c_{o} * k_{h} * k_{w}$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>notations</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$n$</td>
<td>batch size</td>
</tr>
<tr>
<td>$c_{i}/c_{o}$</td>
<td>Input/Output Channels</td>
</tr>
<tr>
<td>$k_{h}/k_{w}$</td>
<td>kernel height/weight</td>
</tr>
<tr>
<td>$g$</td>
<td>group number</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<h2 id="heading"></h2>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
