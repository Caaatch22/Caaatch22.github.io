<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>model-deployment on Mingjie&#39;s Home</title>
    <link>https://caaatch22.github.io/tags/model-deployment/</link>
    <description>Recent content in model-deployment on Mingjie&#39;s Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 27 Feb 2024 22:19:21 +0000</lastBuildDate><atom:link href="https://caaatch22.github.io/tags/model-deployment/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TinyMl —— quantization</title>
      <link>https://caaatch22.github.io/posts/tinyml-quantization/</link>
      <pubDate>Tue, 27 Feb 2024 22:19:21 +0000</pubDate>
      
      <guid>https://caaatch22.github.io/posts/tinyml-quantization/</guid>
      <description>Quantization 模型量化（quantization）指的是用更少的bit表示模型参数，从而减少模型的大小，加速推理过程的技术。
一种常见的量化方式是线性量化(linear quantization)，也叫仿射量化(affine quantization)。其实就是按比例将tensor（一般为fp32）放缩到 $2^{bitwidth}$ 的范围内，比如8bit等。我们很容易给出量化公式： $$ r = s(q - z) $$ 其中，r(real value)值得是量化前的值，q(quantized value)是量化后的值，s(scale)是放缩比例，z(zero point)相当于是一个偏移量。
如何求出$s$和$z$呢？一种简单且常见的方式是通过最大最小值来估计，即：
$$ s = \frac{r_{max} - r_{min}}{q_{max} - q_{min}} $$ $r_{max}$就是这个tensor的最大值，$r_{min}$是最小值，$q_{max}$和$q_{min}$是我们指定的量化后的最大最小值。如下图所示： 有了scale, 容易得到 $z = q_{min} - \frac{r_{min}}{s}$。在实际操作中，z一般会被round到最近的整数$z = round(q_{min} - \frac{r_{min}}{s})$（有很多不同的round规则，这个有具体实现决定）。
得到量化方程： $$ q = clip(round(\frac{r}{s}) + z, q_{min}, q_{max}) $$
代码示意如下：（实际会用pytorch已有的quantize api或者其他推理框架）
def get_quantized_range(bitwidth): quantized_max = (1 &amp;lt;&amp;lt; (bitwidth - 1)) - 1 quantized_min = -(1 &amp;lt;&amp;lt; (bitwidth - 1)) return quantized_min, quantized_max def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.</description>
      <content:encoded><![CDATA[<h2 id="quantization">Quantization</h2>
<p>模型量化（quantization）指的是用更少的bit表示模型参数，从而减少模型的大小，加速推理过程的技术。</p>
<p>一种常见的量化方式是线性量化(linear quantization)，也叫仿射量化(affine quantization)。其实就是按比例将tensor（一般为fp32）放缩到 $2^{bitwidth}$ 的范围内，比如8bit等。我们很容易给出量化公式：
$$
r = s(q - z)
$$
其中，r(real value)值得是量化前的值，q(quantized value)是量化后的值，s(scale)是放缩比例，z(zero point)相当于是一个偏移量。</p>
<p>如何求出$s$和$z$呢？一种简单且常见的方式是通过最大最小值来估计，即：</p>
<p>$$
s = \frac{r_{max} - r_{min}}{q_{max} - q_{min}}
$$
$r_{max}$就是这个tensor的最大值，$r_{min}$是最小值，$q_{max}$和$q_{min}$是我们指定的量化后的最大最小值。如下图所示：
<img loading="lazy" src="/img/tinyml/quantization/quantization.png" alt="image"  />
</p>
<p>有了scale, 容易得到 $z = q_{min} - \frac{r_{min}}{s}$。在实际操作中，z一般会被round到最近的整数$z = round(q_{min} - \frac{r_{min}}{s})$（有很多不同的round规则，这个有具体实现决定）。</p>
<p>得到量化方程：
$$
q = clip(round(\frac{r}{s}) + z, q_{min}, q_{max})
$$</p>
<p>代码示意如下：（实际会用pytorch已有的quantize api或者其他推理框架）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_quantized_range</span>(bitwidth):
</span></span><span style="display:flex;"><span>    quantized_max <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> (bitwidth <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    quantized_min <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> (bitwidth <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quantized_min, quantized_max
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linear_quantize</span>(fp_tensor, bitwidth, scale, zero_point, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int8) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>    rounded_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>round(fp_tensor <span style="color:#f92672">/</span> scale)<span style="color:#f92672">.</span>to(dtype)
</span></span><span style="display:flex;"><span>    shifted_tensor <span style="color:#f92672">=</span> rounded_tensor <span style="color:#f92672">+</span> zero_point
</span></span><span style="display:flex;"><span>    quantized_min, quantized_max <span style="color:#f92672">=</span> get_quantized_range(bitwidth)
</span></span><span style="display:flex;"><span>    quantized_tensor <span style="color:#f92672">=</span> shifted_tensor<span style="color:#f92672">.</span>clamp_(quantized_min, quantized_max)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quantized_tensor
</span></span></code></pre></div><p>上述过程被称为非对称量化(asymmetric quantization)。</p>
<p>还有一种对称量化(symmetric quantization)，它基于以下事实：常见的训练好的模型参数几乎总是关于0对称的。如下图所示：</p>
<p><img loading="lazy" src="/img/tinyml/quantization/weight_distribution.png" alt="weight distribution"  />
</p>
<p>基于这个观察，我们常将zero point设置为0，并让q_{min} = -q_{max}。这样，我们就可以简化量化公式为：
$$
s = \frac{|r_{max}|}{q_{max}} \ z = 0 \
q = clip(round(\frac{r}{s}), -q_{max}, q_{max}), q_{max} = 2^{bitwidth - 1} - 1
$$
这也是TensorRT等框架中常用的量化方式。</p>
<p>进一步的，当我们进行推理的过程中，对于一个全连接层，设$Y = WX + b$，对左右两边都进行量化得到：
$$
S_{Y}(q_{Y} - z_{Y}) = S_{W}(q_{W} - z_{W})\cdot S_{X}(q_{X} - z_{x}) + S_{b}(q_{b} - z_{b})
$$</p>
<p>After quantization, the inference of convolution and fully-connected layers also change.</p>
<p>Recall that $r = S(q-Z)$, and we have</p>
<blockquote>
<p>$r_{\mathrm{input}} = S_{\mathrm{input}}(q_{\mathrm{input}}-Z_{\mathrm{input}})$</p>
<p>$r_{\mathrm{weight}} = S_{\mathrm{weight}}(q_{\mathrm{weight}}-Z_{\mathrm{weight}})$</p>
<p>$r_{\mathrm{bias}} = S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})$</p>
</blockquote>
<p>Since $Z_{\mathrm{weight}}=0$, $r_{\mathrm{weight}} = S_{\mathrm{weight}}q_{\mathrm{weight}}$.</p>
<p>The floating point convolution can be written as,</p>
<blockquote>
<p>$r_{\mathrm{output}} = \mathrm{CONV}[r_{\mathrm{input}}, r_{\mathrm{weight}}] + r_{\mathrm{bias}}\
;;;;;;;;= \mathrm{CONV}[S_{\mathrm{input}}(q_{\mathrm{input}}-Z_{\mathrm{input}}), S_{\mathrm{weight}}q_{\mathrm{weight}}] + S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})\
;;;;;;;;= \mathrm{CONV}[q_{\mathrm{input}}-Z_{\mathrm{input}}, q_{\mathrm{weight}}]\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}}) + S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})$</p>
</blockquote>
<p>To further simplify the computation, we could let</p>
<blockquote>
<p>$Z_{\mathrm{bias}} = 0$</p>
<p>$S_{\mathrm{bias}} = S_{\mathrm{input}} \cdot S_{\mathrm{weight}}$</p>
</blockquote>
<p>so that</p>
<blockquote>
<p>$r_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}-Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}})$
$;;;;;;;;= (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}}S_{\mathrm{weight}})$</p>
</blockquote>
<p>Since</p>
<blockquote>
<p>$r_{\mathrm{output}} = S_{\mathrm{output}}(q_{\mathrm{output}}-Z_{\mathrm{output}})$</p>
</blockquote>
<p>we have</p>
<blockquote>
<p>$S_{\mathrm{output}}(q_{\mathrm{output}}-Z_{\mathrm{output}}) = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} S_{\mathrm{weight}})$</p>
</blockquote>
<p>and thus</p>
<blockquote>
<p>$q_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}}S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}$</p>
</blockquote>
<p>Since $Z_{\mathrm{input}}$, $q_{\mathrm{weight}}$, $q_{\mathrm{bias}}$ are determined before inference, let</p>
<blockquote>
<p>$Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]$</p>
</blockquote>
<p>we have</p>
<blockquote>
<p>$q_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}}) \cdot (S_{\mathrm{input}}S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}$</p>
</blockquote>
<p>Similarily, for fully-connected layer, we have</p>
<blockquote>
<p>$q_{\mathrm{output}} = (\mathrm{Linear}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}$</p>
</blockquote>
<p>where</p>
<blockquote>
<p>$Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{Linear}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]$</p>
</blockquote>
<h2 id="post-training-quantizationptq">Post-training quantization(PTQ)</h2>
<p>$$
q_{output} = (linear(q_{input}, q_{weight}) + Q_{bias}) * (s_{input} * s_{weight} / s_{output})
$$</p>
<h2 id="qauntization-aware-trainingqat">Qauntization-aware training(QAT)</h2>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
