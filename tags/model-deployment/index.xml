<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>model-deployment on Mingjie&#39;s Home</title>
    <link>https://caaatch22.github.io/tags/model-deployment/</link>
    <description>Recent content in model-deployment on Mingjie&#39;s Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 27 Feb 2024 22:19:21 +0000</lastBuildDate><atom:link href="https://caaatch22.github.io/tags/model-deployment/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TinyMl —— 模型量化(quantization)</title>
      <link>https://caaatch22.github.io/posts/tinyml-quantization/</link>
      <pubDate>Tue, 27 Feb 2024 22:19:21 +0000</pubDate>
      
      <guid>https://caaatch22.github.io/posts/tinyml-quantization/</guid>
      <description>Overview 模型量化（quantization）指的是用更少的bit表示模型参数，从而减少模型的大小，加速推理过程的技术。
一种常见的量化方式是线性量化(linear quantization)，也叫仿射量化(affine quantization)。其实就是按比例将tensor（一般为fp32）放缩到 $2^{bitwidth}$ 的范围内，比如8bit等。我们很容易给出量化公式： $$ r = s(q - z) $$ 其中，r(real value)值得是量化前的值，q(quantized value)是量化后的值，s(scale)是放缩比例，z(zero point)相当于是一个偏移量。
如何求出$s$和$z$呢？一种简单且常见的方式是通过最大最小值来估计，即：
$$ s = \frac{r_{max} - r_{min}}{q_{max} - q_{min}} $$ $r_{max}$就是这个tensor的最大值，$r_{min}$是最小值，$q_{max}$和$q_{min}$是我们指定的量化后的最大最小值。如下图所示： 有了scale, 容易得到 $z = q_{min} - \frac{r_{min}}{s}$。在实际操作中，z一般会被round到最近的整数$z = round(q_{min} - \frac{r_{min}}{s})$（有很多不同的round规则，这个有具体实现决定）。
得到量化方程： $$ q = clip(round(\frac{r}{s}) + z, q_{min}, q_{max}) $$
代码示意如下：（实际会用pytorch已有的quantize api或者其他推理框架）
def get_quantized_range(bitwidth): quantized_max = (1 &amp;lt;&amp;lt; (bitwidth - 1)) - 1 quantized_min = -(1 &amp;lt;&amp;lt; (bitwidth - 1)) return quantized_min, quantized_max def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.</description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>模型量化（quantization）指的是用更少的bit表示模型参数，从而减少模型的大小，加速推理过程的技术。</p>
<p>一种常见的量化方式是线性量化(linear quantization)，也叫仿射量化(affine quantization)。其实就是按比例将tensor（一般为fp32）放缩到 $2^{bitwidth}$ 的范围内，比如8bit等。我们很容易给出量化公式：
$$
r = s(q - z)
$$
其中，r(real value)值得是量化前的值，q(quantized value)是量化后的值，s(scale)是放缩比例，z(zero point)相当于是一个偏移量。</p>
<p>如何求出$s$和$z$呢？一种简单且常见的方式是通过最大最小值来估计，即：</p>
<p>$$
s = \frac{r_{max} - r_{min}}{q_{max} - q_{min}}
$$
$r_{max}$就是这个tensor的最大值，$r_{min}$是最小值，$q_{max}$和$q_{min}$是我们指定的量化后的最大最小值。如下图所示：
<img loading="lazy" src="/img/tinyml/quantization/quantization.png" alt="image"  />
</p>
<p>有了scale, 容易得到 $z = q_{min} - \frac{r_{min}}{s}$。在实际操作中，z一般会被round到最近的整数$z = round(q_{min} - \frac{r_{min}}{s})$（有很多不同的round规则，这个有具体实现决定）。</p>
<p>得到量化方程：
$$
q = clip(round(\frac{r}{s}) + z, q_{min}, q_{max})
$$</p>
<p>代码示意如下：（实际会用pytorch已有的quantize api或者其他推理框架）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_quantized_range</span>(bitwidth):
</span></span><span style="display:flex;"><span>    quantized_max <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> (bitwidth <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    quantized_min <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> (bitwidth <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quantized_min, quantized_max
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linear_quantize</span>(fp_tensor, bitwidth, scale, zero_point, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int8) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>    rounded_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>round(fp_tensor <span style="color:#f92672">/</span> scale)<span style="color:#f92672">.</span>to(dtype)
</span></span><span style="display:flex;"><span>    shifted_tensor <span style="color:#f92672">=</span> rounded_tensor <span style="color:#f92672">+</span> zero_point
</span></span><span style="display:flex;"><span>    quantized_min, quantized_max <span style="color:#f92672">=</span> get_quantized_range(bitwidth)
</span></span><span style="display:flex;"><span>    quantized_tensor <span style="color:#f92672">=</span> shifted_tensor<span style="color:#f92672">.</span>clamp_(quantized_min, quantized_max)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quantized_tensor
</span></span></code></pre></div><p>上述过程被称为非对称量化(asymmetric quantization)。</p>
<p>还有一种对称量化(symmetric quantization)，它基于以下事实：常见的训练好的模型参数几乎总是关于0对称的。如下图所示：</p>
<p><img loading="lazy" src="/img/tinyml/quantization/weight_distribution.png" alt="weight distribution"  />
</p>
<p>基于这个观察，我们常将zero point设置为0，并让 $q_{min} = -q_{max}$ 。这样，我们就可以简化量化公式为：
$$
s = \frac{r_{max}}{q_{max}},\ \ z = 0 \
q = clip(round(\frac{r}{s}), -q_{max}, q_{max}), q_{max} = 2^{bitwidth - 1} - 1
$$
这也是TensorRT等框架中常用的量化方式。</p>
<p>进一步的，当我们进行推理的过程中，对于一个全连接层，设$Y = WX$（暂不考虑bias），对左右两边都进行量化得到：</p>
<p>$$
\begin{align*}
S_{Y}(q_{Y} - z_{Y}) &amp;= S_{W}(q_{W} - z_{W})\cdot S_{X}(q_{X} - z_{x}) \newline
q_{Y} &amp;= \frac{S_{W}S_{X}}{S_{Y}}(q_{W} - z_{W})(q_{x} - z_{x}) + z_{Y} \newline
q_{Y} &amp;= \frac{S_{W}S_{X}}{S_{Y}}(q_{W}q_{x} - q_{W}z_{x} - z_{W}q_{x} + z_{W}z_{x}) + z_{Y} \newline
q_{Y} &amp;= \frac{S_{W}S_{X}}{S_{Y}}(q_{W}q_{X} - z_{X}q_{W}) + z_{Y}\ \ (assume\ z_{W} = 0) \newline
\end{align*}
$$</p>
<p>类似的，有bias的时候可以得到：
$$
q_{Y} = \frac{S_{W}S_{X}}{S_{Y}}(q_{W}q_{X} - Q_{bias}) + z_{Y} \ \ \ (Q_{bias} = q_{b} - z_{X}q_{W})
$$
根据卷积的线性可加性，我们也能有类似的结论：
<img loading="lazy" src="/img/tinyml/quantization/conv_quantization.png" alt="conv quantization"  />

推理过程和全连接层类似，也可参见tinyml课程的lab2作业。</p>
<p>上式中，和 $W, b$ 相关的量化参数都可以容易被提前计算（训练好后对整个网络进行量化，记录所有weight, bias的量化值）。但对于$q_{X}， z_{X}$这种和输入相关的值，理论上来说是需要真正在设备上做推理的时候在能知道。但是如果我们每次做推理来一个input我们都对其做一个量化，这个开销是没法接受的。所以人们会用一个<strong>cablibration dataset</strong>，在做推理之前，在这个数据集上对所有的输入进行量化并得到一些量化参数，比如，cablibration dataset上的最大最小值，scale, zero point， 以及$S_{Y}$等（具体过程见下文中的PTQ和QAT），以此当作推理时input的量化参数。</p>
<p>上述式子还存在两个问题</p>
<ol>
<li>$\frac{S_{W}S_{X}}{S_{Y}}$是一个浮点数，我们不能让它参与计算。根据<em>经验</em>，这个结果一般都在(0, 1)之间，所以可以表示成$2^{-n}M_{0}$， 其中$M_{0}$是一个整数，$n$是一个非负整数。这样我们就可以记录两个整数来代替浮点数。可以参见<a href="https://github.com/google/gemmlowp/blob/master/doc/quantization_example.cc#L210">gemmlowp的实现</a></li>
<li>$q_{W} \cdot q_{X}$ 是很可能溢出 8bit的，所以其结果一般也会用32bit int表示（具体可能有不同的实现），所以我们一般也先将$Q_{bias}$量化为32bit int，便于与其结果相乘。最后需要对结果在量化为8bit int。</li>
</ol>
<h3 id="其他量化方法">其他量化方法</h3>
<p>minmax量化（$s = \frac{r_{max}-r_{min}}{q_{max} - q_{min}}$）有一个问题，也就是它容易受outlier的点的影响，这对模型参数的量化其实影响还好，因为参数的分布基本是对称的。但是对activations的结果就不一样了，所以又衍生出了几种量化方式。还记得上文说过，为了在模型真正部署之前得到对input, activations的量化参数，我们会在一个cablibration dataset进行训练。在这个过程中，我们可以通过取平均或者*指数移动平均(exponential moving averages)*的方式获取 $r_{max}, r_{min}$ ，从而减少outlier的影响。
$$
\hat{r}^{(t)}<em>{max, min} = \alpha \cdot r^{(t)}</em>{max, min} + (1 - \alpha) \cdot \hat{r}^{(t-1)}_{max, min}
$$</p>
<p>TensorRT对activations的量化其实也是通过minmax方法，但是这个minmax是在一定阈值之内的minmax。
<img loading="lazy" src="/img/tinyml/quantization/tensorrt_threshold.png" alt=""  />

那么如何确定这个阈值呢？我们肯定希望<strong>最小化量化前后数据分布的信息损失</strong>，这也是KL散度的思想。操作过程如下：</p>
<p>对于模型的每一层：</p>
<ol>
<li>计算activations的直方图</li>
<li>选取多种threshold计算梁化后的分布与原分布的KL散度</li>
<li>选取最小KL散度对应的threshold
整个过程在典型的工作负载下大概需要几分钟。</li>
</ol>
<p>一些选取的threshold结果如下：
<img loading="lazy" src="/img/tinyml/quantization/tensorrt_quantization_threshold.png" alt="tensorrt threshold"  />
</p>
<h3 id="量化粒度">量化粒度</h3>
<p>我们可以对每个tensor进行量化，也即对每个tensor都有一个scale和zero point。但人们发现，同一个tensor的不同channel的分布是很不一样的：
<img loading="lazy" src="/img/tinyml/quantization/channel_distribution.png" alt="channel distribution"  />

所以一个更细粒度的选择是进行per-channel量化，即对每个channel都有一个scale和zero point。</p>
<h2 id="post-training-quantizationptq">Post-training quantization(PTQ)</h2>
<p>PTQ的过程比较简单，就是在训练后对模型进行量化。在这个过程前，我们还会用cablibration dataset来估计一些量化参数。流程如下：
<img loading="lazy" src="/img/tinyml/quantization/PTQ.png" alt="ptq"  />
</p>
<p>简单的pytorch代码如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QuantizedSimpleNet</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_size_1<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, hidden_size_2<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
</span></span><span style="display:flex;"><span>        super(QuantizedSimpleNet,self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>quant <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>QuantStub()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">28</span><span style="color:#f92672">*</span><span style="color:#ae81ff">28</span>, hidden_size_1) 
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_size_1, hidden_size_2) 
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_size_2, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dequant <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>DeQuantStub()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, img):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">28</span><span style="color:#f92672">*</span><span style="color:#ae81ff">28</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>quant(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>linear1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>linear2(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear3(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dequant(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>net_quantized <span style="color:#f92672">=</span> QuantizedSimpleNet()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Copy weights from unquantized model</span>
</span></span><span style="display:flex;"><span>net_quantized<span style="color:#f92672">.</span>load_state_dict(net<span style="color:#f92672">.</span>state_dict())
</span></span><span style="display:flex;"><span>net_quantized<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>net_quantized<span style="color:#f92672">.</span>qconfig <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ao<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>default_qconfig
</span></span><span style="display:flex;"><span>net_quantized <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ao<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>prepare(net_quantized) <span style="color:#75715e"># Insert observers</span>
</span></span><span style="display:flex;"><span>print(net_quantized)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># QuantizedVerySimpleNet(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (quant): QuantStub(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear1): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=784, out_features=100, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear2): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=100, out_features=100, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear3): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=100, out_features=10, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (relu): ReLU()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (dequant): DeQuantStub()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># )</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 这次的测试实际上是做cablibration</span>
</span></span><span style="display:flex;"><span>test(net_quantized)
</span></span><span style="display:flex;"><span>print(net_quantized)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># QuantizedVerySimpleNet(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (quant): QuantStub(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear1): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=784, out_features=100, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=-53.58397674560547, max_val=34.898128509521484)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear2): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=100, out_features=100, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=-24.331275939941406, max_val=26.62542152404785)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear3): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=100, out_features=10, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=-28.273700714111328, max_val=20.937761306762695)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (relu): ReLU()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (dequant): DeQuantStub()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># )</span>
</span></span><span style="display:flex;"><span>net_quantized <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ao<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>convert(net_quantized)
</span></span><span style="display:flex;"><span>print(net_quantized)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># QuantizedVerySimpleNet(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.6967094540596008, zero_point=77, qscheme=torch.per_tensor_affine)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.40123382210731506, zero_point=61, qscheme=torch.per_tensor_affine)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.3874918520450592, zero_point=73, qscheme=torch.per_tensor_affine)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (relu): ReLU()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (dequant): DeQuantize()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># )</span>
</span></span></code></pre></div><p>在实际的部署中，一般不会用pytorch的量化模块。根据你所需要的后端，选择tensorrt, onnxruntime, openvino, ncnn等框架的量化模块。</p>
<h2 id="qauntization-aware-trainingqat">Qauntization-aware training(QAT)</h2>
<p>qat顾名思义，指的是开模型训练的前就将模型进行量化，从而训练出来的误差更接近“量化误差”。但人们经过广泛的时间发现，将一个训练好的模型在量化后进行微调，要比量化模型在从零训练的准确率更高。所以现在的qat的training一般指的是微调量化模型。
在pytorch中的流程基本和PTQ差不多，就是训练时改成</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>net<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>net_quantized <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ao<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>prepare_qat(net) <span style="color:#75715e"># Insert observers</span>
</span></span></code></pre></div><p>但是其原理不同，因为涉及到了反向传播的过程。QAT的流程如下：
<img loading="lazy" src="/img/tinyml/quantization/QAT.png" alt="quantization aware training"  />
</p>
<p>在qat的过程中，我们会保存一份fp32的weight副本，这是用来更新梯度的（若只有梁化后的int8 weight，我们每次的梯度变化会在round的时候被归零，例如，weight是3.2， quantized weight是3，这次反向传播的时候需要剪去0.1，如果quantized weight - 0.1在round之后还是3，所以我们需要保留原来的fp32 weight, 不断累积梯度更新，当累计一定值后，quantized weight就会被更新了）。</p>
<p>在反向传播过程中，模型需要对每个权重和输入计算损失函数的梯度。这里出现了一个问题：我们之前定义的量化操作的导数是什么？实际上量化得到的结果是离散值，所以导数应该在任何地方都是0， $\frac{\partial Q(W)}{\partial W} = 0$，如果我们这个设置，网络什么也学不到因为梯度得不到更新。
一个典型的解决方案是使用<strong>Straight-through Estimator</strong>，简称STE，来近似这个梯度。STE简单的将量化操作的导数设置为1，即$\frac{\partial Q(W)}{\partial W} = 1$。这样，我们就可以在反向传播的过程中更新梯度了。为什么能设置成1呢，因为实际上这是一个阶梯函数，如下图所示：
<img loading="lazy" src="/img/tinyml/quantization/STE.png" alt="ste"  />
</p>
<h2 id="reference">reference</h2>
<ul>
<li><a href="https://hanlab.mit.edu/courses/2023-fall-65940">tinyml quantization</a></li>
<li><a href="https://arxiv.org/abs/1806.08342">Quantizing deep convolutional networks for efficient inference: A whitepaper</a></li>
<li><a href="https://github.com/google/gemmlowp/blob/master/doc/quantization_example.cc">gemmlowp</a>关于量化的示例</li>
<li><a href="https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">8-bit inference with TensorRT</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
