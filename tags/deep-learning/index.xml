<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>deep-learning on Mingjie&#39;s Home</title>
    <link>https://caaatch22.github.io/tags/deep-learning/</link>
    <description>Recent content in deep-learning on Mingjie&#39;s Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 27 Feb 2024 22:19:21 +0000</lastBuildDate><atom:link href="https://caaatch22.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TinyMl —— quantization</title>
      <link>https://caaatch22.github.io/posts/tinyml-quantization/</link>
      <pubDate>Tue, 27 Feb 2024 22:19:21 +0000</pubDate>
      
      <guid>https://caaatch22.github.io/posts/tinyml-quantization/</guid>
      <description>Overview 模型量化（quantization）指的是用更少的bit表示模型参数，从而减少模型的大小，加速推理过程的技术。
一种常见的量化方式是线性量化(linear quantization)，也叫仿射量化(affine quantization)。其实就是按比例将tensor（一般为fp32）放缩到 $2^{bitwidth}$ 的范围内，比如8bit等。我们很容易给出量化公式： $$ r = s(q - z) $$ 其中，r(real value)值得是量化前的值，q(quantized value)是量化后的值，s(scale)是放缩比例，z(zero point)相当于是一个偏移量。
如何求出$s$和$z$呢？一种简单且常见的方式是通过最大最小值来估计，即：
$$ s = \frac{r_{max} - r_{min}}{q_{max} - q_{min}} $$ $r_{max}$就是这个tensor的最大值，$r_{min}$是最小值，$q_{max}$和$q_{min}$是我们指定的量化后的最大最小值。如下图所示： 有了scale, 容易得到 $z = q_{min} - \frac{r_{min}}{s}$。在实际操作中，z一般会被round到最近的整数$z = round(q_{min} - \frac{r_{min}}{s})$（有很多不同的round规则，这个有具体实现决定）。
得到量化方程： $$ q = clip(round(\frac{r}{s}) + z, q_{min}, q_{max}) $$
代码示意如下：（实际会用pytorch已有的quantize api或者其他推理框架）
def get_quantized_range(bitwidth): quantized_max = (1 &amp;lt;&amp;lt; (bitwidth - 1)) - 1 quantized_min = -(1 &amp;lt;&amp;lt; (bitwidth - 1)) return quantized_min, quantized_max def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.</description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>模型量化（quantization）指的是用更少的bit表示模型参数，从而减少模型的大小，加速推理过程的技术。</p>
<p>一种常见的量化方式是线性量化(linear quantization)，也叫仿射量化(affine quantization)。其实就是按比例将tensor（一般为fp32）放缩到 $2^{bitwidth}$ 的范围内，比如8bit等。我们很容易给出量化公式：
$$
r = s(q - z)
$$
其中，r(real value)值得是量化前的值，q(quantized value)是量化后的值，s(scale)是放缩比例，z(zero point)相当于是一个偏移量。</p>
<p>如何求出$s$和$z$呢？一种简单且常见的方式是通过最大最小值来估计，即：</p>
<p>$$
s = \frac{r_{max} - r_{min}}{q_{max} - q_{min}}
$$
$r_{max}$就是这个tensor的最大值，$r_{min}$是最小值，$q_{max}$和$q_{min}$是我们指定的量化后的最大最小值。如下图所示：
<img loading="lazy" src="/img/tinyml/quantization/quantization.png" alt="image"  />
</p>
<p>有了scale, 容易得到 $z = q_{min} - \frac{r_{min}}{s}$。在实际操作中，z一般会被round到最近的整数$z = round(q_{min} - \frac{r_{min}}{s})$（有很多不同的round规则，这个有具体实现决定）。</p>
<p>得到量化方程：
$$
q = clip(round(\frac{r}{s}) + z, q_{min}, q_{max})
$$</p>
<p>代码示意如下：（实际会用pytorch已有的quantize api或者其他推理框架）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_quantized_range</span>(bitwidth):
</span></span><span style="display:flex;"><span>    quantized_max <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> (bitwidth <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    quantized_min <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> (bitwidth <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quantized_min, quantized_max
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linear_quantize</span>(fp_tensor, bitwidth, scale, zero_point, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int8) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>    rounded_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>round(fp_tensor <span style="color:#f92672">/</span> scale)<span style="color:#f92672">.</span>to(dtype)
</span></span><span style="display:flex;"><span>    shifted_tensor <span style="color:#f92672">=</span> rounded_tensor <span style="color:#f92672">+</span> zero_point
</span></span><span style="display:flex;"><span>    quantized_min, quantized_max <span style="color:#f92672">=</span> get_quantized_range(bitwidth)
</span></span><span style="display:flex;"><span>    quantized_tensor <span style="color:#f92672">=</span> shifted_tensor<span style="color:#f92672">.</span>clamp_(quantized_min, quantized_max)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quantized_tensor
</span></span></code></pre></div><p>上述过程被称为非对称量化(asymmetric quantization)。</p>
<p>还有一种对称量化(symmetric quantization)，它基于以下事实：常见的训练好的模型参数几乎总是关于0对称的。如下图所示：</p>
<p><img loading="lazy" src="/img/tinyml/quantization/weight_distribution.png" alt="weight distribution"  />
</p>
<p>基于这个观察，我们常将zero point设置为0，并让q_{min} = -q_{max}。这样，我们就可以简化量化公式为：
$$
s = \frac{r_{max}}{q_{max}},\ \ z = 0 \
q = clip(round(\frac{r}{s}), -q_{max}, q_{max}), q_{max} = 2^{bitwidth - 1} - 1
$$
这也是TensorRT等框架中常用的量化方式。</p>
<p>进一步的，当我们进行推理的过程中，对于一个全连接层，设$Y = WX$（暂不考虑bias），对左右两边都进行量化得到：</p>
<p>$$
\begin{align*}
S_{Y}(q_{Y} - z_{Y}) &amp;= S_{W}(q_{W} - z_{W})\cdot S_{X}(q_{X} - z_{x}) \newline
q_{Y} &amp;= \frac{S_{W}S_{X}}{S_{Y}}(q_{W} - z_{W})(q_{x} - z_{x}) + z_{Y} \newline
q_{Y} &amp;= \frac{S_{W}S_{X}}{S_{Y}}(q_{W}q_{x} - q_{W}z_{x} - z_{W}q_{x} + z_{W}z_{x}) + z_{Y} \newline
q_{Y} &amp;= \frac{S_{W}S_{X}}{S_{Y}}(q_{W}q_{X} - z_{X}q_{W}) + z_{Y}\ \ (assume\ z_{W} = 0) \newline
\end{align*}
$$</p>
<p>类似的，有bias的时候可以得到：
$$
q_{Y} = \frac{S_{W}S_{X}}{S_{Y}}(q_{W}q_{X} - Q_{bias}) + z_{Y} \ \ \ (Q_{bias} = q_{b} - z_{X}q_{W})
$$
根据卷积的线性可加性，我们也能有类似的结论：
<img loading="lazy" src="/img/tinyml/quantization/conv_quantization.png" alt="conv quantization"  />

推理过程和全连接层类似，也可参见tinyml课程的lab2作业。</p>
<p>上式中，和 $W, b$ 相关的量化参数都可以容易被提前计算（训练好后对整个网络进行量化，记录所有weight, bias的量化值）。但对于$q_{X}， z_{X}$这种和输入相关的值，理论上来说是需要真正在设备上做推理的时候在能知道。但是如果我们每次做推理来一个input我们都对其做一个量化，这个开销是没法接受的。所以人们会用一个<strong>cablibration dataset</strong>，在做推理之前，在这个数据集上对所有的输入进行量化并得到一些量化参数，比如，cablibration dataset上的最大最小值，scale, zero point， 以及$S_{Y}$等（具体过程见下文中的PTQ和QAT），以此当作推理时input的量化参数。</p>
<p>上述式子还存在两个问题</p>
<ol>
<li>$\frac{S_{W}S_{X}}{S_{Y}}$是一个浮点数，我们不能让它参与计算。根据<em>经验</em>，这个结果一般都在(0, 1)之间，所以可以表示成$2^{-n}M_{0}$， 其中$M_{0}$是一个整数，$n$是一个非负整数。这样我们就可以记录两个整数来代替浮点数。可以参见<a href="https://github.com/google/gemmlowp/blob/master/doc/quantization_example.cc#L210">gemmlowp的实现</a></li>
<li>$q_{W} \cdot q_{X}$ 是很可能溢出 8bit的，所以其结果一般也会用32bit int表示（具体可能有不同的实现），所以我们一般也先将$Q_{bias}$量化为32bit int，便于与其结果相乘。最后需要对结果在量化为8bit int。</li>
</ol>
<h3 id="其他量化方法">其他量化方法</h3>
<p>minmax量化（$s = \frac{r_{max}-r_{min}}{q_{max} - q_{min}}$）有一个问题，也就是它容易受outlier的点的影响，这对模型参数的量化其实影响还好，因为参数的分布基本是对称的。但是对activations的结果就不一样了，所以又衍生出了几种量化方式。还记得上文说过，为了在模型真正部署之前得到对input, activations的量化参数，我们会在一个cablibration dataset进行训练。在这个过程中，我们可以通过取平均或者*指数移动平均(exponential moving averages)*的方式获取r_{max}, r_{min}，从而减少outlier的影响。
$$
\hat{r}^{(t)}<em>{max, min} = \alpha \cdot r^{(t)}</em>{max, min} + (1 - \alpha) \cdot \hat{r}^{(t-1)}_{max, min}
$$</p>
<p>TensorRT对activations的量化其实也是通过minmax方法，但是这个minmax是在一定阈值之内的minmax。
<img loading="lazy" src="/img/tinyml/quantization/tensorrt_threshold.png" alt=""  />

那么如何确定这个阈值呢？我们肯定希望<strong>最小化量化前后数据分布的信息损失</strong>，这也是KL散度的思想。操作过程如下：</p>
<p>对于模型的每一层：</p>
<ol>
<li>计算activations的直方图</li>
<li>选取多种threshold计算梁化后的分布与原分布的KL散度</li>
<li>选取最小KL散度对应的threshold
整个过程在典型的工作负载下大概需要几分钟。</li>
</ol>
<p>一些选取的threshold结果如下：
<img loading="lazy" src="/img/tinyml/quantization/tensorrt_quantization_threshold.png" alt="tensorrt threshold"  />
</p>
<h3 id="量化粒度">量化粒度</h3>
<p>我们可以对每个tensor进行量化，也即对每个tensor都有一个scale和zero point。但人们发现，同一个tensor的不同channel的分布是很不一样的：
<img loading="lazy" src="/img/tinyml/quantization/channel_distribution.png" alt="channel distribution"  />

所以一个更细粒度的选择是进行per-channel量化，即对每个channel都有一个scale和zero point。</p>
<h2 id="post-training-quantizationptq">Post-training quantization(PTQ)</h2>
<p>PTQ的过程比较简单，就是在训练后对模型进行量化。在这个过程前，我们还会用cablibration dataset来估计一些量化参数。流程如下：
<img loading="lazy" src="/img/tinyml/quantization/PTQ.png" alt="ptq"  />
</p>
<p>简单的pytorch代码如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QuantizedSimpleNet</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_size_1<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, hidden_size_2<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>):
</span></span><span style="display:flex;"><span>        super(QuantizedSimpleNet,self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>quant <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>QuantStub()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">28</span><span style="color:#f92672">*</span><span style="color:#ae81ff">28</span>, hidden_size_1) 
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_size_1, hidden_size_2) 
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_size_2, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dequant <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>DeQuantStub()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, img):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">28</span><span style="color:#f92672">*</span><span style="color:#ae81ff">28</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>quant(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>linear1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>linear2(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear3(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dequant(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>net_quantized <span style="color:#f92672">=</span> QuantizedSimpleNet()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Copy weights from unquantized model</span>
</span></span><span style="display:flex;"><span>net_quantized<span style="color:#f92672">.</span>load_state_dict(net<span style="color:#f92672">.</span>state_dict())
</span></span><span style="display:flex;"><span>net_quantized<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>net_quantized<span style="color:#f92672">.</span>qconfig <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ao<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>default_qconfig
</span></span><span style="display:flex;"><span>net_quantized <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ao<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>prepare(net_quantized) <span style="color:#75715e"># Insert observers</span>
</span></span><span style="display:flex;"><span>print(net_quantized)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># QuantizedVerySimpleNet(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (quant): QuantStub(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear1): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=784, out_features=100, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear2): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=100, out_features=100, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear3): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=100, out_features=10, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (relu): ReLU()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (dequant): DeQuantStub()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># )</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 这次的测试实际上是做cablibration</span>
</span></span><span style="display:flex;"><span>test(net_quantized)
</span></span><span style="display:flex;"><span>print(net_quantized)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># QuantizedVerySimpleNet(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (quant): QuantStub(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear1): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=784, out_features=100, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=-53.58397674560547, max_val=34.898128509521484)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear2): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=100, out_features=100, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=-24.331275939941406, max_val=26.62542152404785)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear3): Linear(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     in_features=100, out_features=10, bias=True</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     (activation_post_process): MinMaxObserver(min_val=-28.273700714111328, max_val=20.937761306762695)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   )</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (relu): ReLU()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (dequant): DeQuantStub()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># )</span>
</span></span><span style="display:flex;"><span>net_quantized <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ao<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>convert(net_quantized)
</span></span><span style="display:flex;"><span>print(net_quantized)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># QuantizedVerySimpleNet(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.6967094540596008, zero_point=77, qscheme=torch.per_tensor_affine)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.40123382210731506, zero_point=61, qscheme=torch.per_tensor_affine)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.3874918520450592, zero_point=73, qscheme=torch.per_tensor_affine)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (relu): ReLU()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   (dequant): DeQuantize()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># )</span>
</span></span></code></pre></div><p>在实际的部署中，一般不会用pytorch的量化模块。根据你所需要的后端，选择tensorrt, onnxruntime, openvino, ncnn等框架的量化模块。</p>
<h2 id="qauntization-aware-trainingqat">Qauntization-aware training(QAT)</h2>
<p>qat顾名思义，指的是开模型训练的前就将模型进行量化，从而训练出来的误差更接近“量化误差”。但人们经过广泛的时间发现，将一个训练好的模型在量化后进行微调，要比量化模型在从零训练的准确率更高。所以现在的qat的training一般指的是微调量化模型。
在pytorch中的流程基本和PTQ差不多，就是训练时改成</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>net<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>net_quantized <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ao<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>prepare_qat(net) <span style="color:#75715e"># Insert observers</span>
</span></span></code></pre></div><p>但是其原理不同，因为涉及到了反向传播的过程。QAT的过程如下：
<img loading="lazy" src="img/tinyml/quantization/QAT.png" alt="quantization aware training"  />
</p>
<h2 id="reference">reference</h2>
<ul>
<li><a href="https://hanlab.mit.edu/courses/2023-fall-65940">tinyml quantization</a></li>
<li><a href="https://arxiv.org/abs/1806.08342">Quantizing deep convolutional networks for efficient inference: A whitepaper</a></li>
<li><a href="https://github.com/google/gemmlowp/blob/master/doc/quantization_example.cc">gemmlowp</a>关于量化的示例</li>
<li><a href="https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">8-bit inference with TensorRT</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>TinyMl —— pruning</title>
      <link>https://caaatch22.github.io/posts/tinyml-pruning/</link>
      <pubDate>Tue, 20 Feb 2024 15:35:59 +0000</pubDate>
      
      <guid>https://caaatch22.github.io/posts/tinyml-pruning/</guid>
      <description>最近正在学习 MIT 6.5940, 韩松老师的课，做deep learning compression的应该都只知道。课程分为三个部分，efficient inference, domain-specific optimization, efficient training。有完整的课件，视频和实验。最后一个lab是将llama2部署在个人电脑上，非常有意思（谁不想要个自己的大模型呢）。其余lab也都可以白嫖google colab的gpu
Introduction 正式介绍pruning and sparsity之前，我们先来聊聊为什么要做model compression这个事情。 Today&amp;rsquo;s Model is Too Big!
随着Large language model的出现，如GPT-3，如今的模型参数量已经达到了上百billion，别说训练，我们甚至无法在一个gpu上对其进行推理。更别提如果我们想要将其部署在其他边缘设备上。
所以当前在做inference之前，一般都会有个model-compression的过程，包括pruning（剪枝），quantization（量化），distillation（蒸馏）等。这些方法都是为了减少模型的大小，加速推理过程。这些方法也被广泛地集成到了各种加速卡，gpu中。例如nv的A100就支持structured sparsity（[N:M]形式的，具体含义下文会详细介绍）。
Efficiency Metrics 我们再来看看一些 efficiency metrics，这也是我们在做inference过程中需要考虑的指标：
Memory-Related Metrics # parameters model size total/peak #activations Computation-Related Metrics MACs FLOP, FLOPs # parameters 下表是一些常见结构的参数数量：
Model #Parameters Linear Layer(FC) $feature_{in} * feature_{out}$ Conv Layer $c_{i} * c_{o} * k_{h} * k_{w} $ Grouped Conv Layer $c_{i} * c_{o} * k_{h} * k_{w} / g$ Depthwise Conv Layer $c_{o} * k_{h} * k_{w}$ 其中，Grouped Conv指的是将输入在channel维度进行分组，然后分别进行卷积，最后concatenate。Depthwise Conv是分组个数 $g$ 等于输入channel数的情况。</description>
      <content:encoded><![CDATA[<blockquote>
<p>最近正在学习 <a href="https://hanlab.mit.edu/courses/2023-fall-65940">MIT 6.5940</a>, <a href="https://hanlab.mit.edu/songhan">韩松</a>老师的课，做deep learning compression的应该都只知道。课程分为三个部分，<strong>efficient inference, domain-specific optimization, efficient training</strong>。有完整的课件，视频和实验。最后一个lab是将llama2部署在个人电脑上，非常有意思（谁不想要个自己的大模型呢）。其余lab也都可以白嫖google colab的gpu</p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>正式介绍pruning and sparsity之前，我们先来聊聊为什么要做model compression这个事情。
<img loading="lazy" src="/img/tinyml/pruning/todays-model-size.png" alt="todays-model-size"  />
</p>
<p><strong>Today&rsquo;s Model is Too Big!</strong></p>
<p>随着Large language model的出现，如GPT-3，如今的模型参数量已经达到了上百billion，别说训练，我们甚至无法在一个gpu上对其进行推理。更别提如果我们想要将其部署在其他边缘设备上。</p>
<p>所以当前在做inference之前，一般都会有个model-compression的过程，包括pruning（剪枝），quantization（量化），distillation（蒸馏）等。这些方法都是为了减少模型的大小，加速推理过程。这些方法也被广泛地集成到了各种加速卡，gpu中。例如nv的A100就支持structured sparsity（[N:M]形式的，具体含义下文会详细介绍）。</p>
<h3 id="efficiency-metrics">Efficiency Metrics</h3>
<p>我们再来看看一些 efficiency metrics，这也是我们在做inference过程中需要考虑的指标：</p>
<ul>
<li>Memory-Related Metrics
<ul>
<li># parameters</li>
<li>model size</li>
</ul>
</li>
<li>total/peak #activations</li>
<li>Computation-Related Metrics
<ul>
<li>MACs</li>
<li>FLOP, FLOPs</li>
</ul>
</li>
</ul>
<h4 id="-parameters"># parameters</h4>
<p>下表是一些常见结构的参数数量：</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>#Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Layer(FC)</td>
<td>$feature_{in} * feature_{out}$</td>
</tr>
<tr>
<td>Conv Layer</td>
<td>$c_{i} * c_{o} * k_{h} * k_{w} $</td>
</tr>
<tr>
<td>Grouped Conv Layer</td>
<td>$c_{i} * c_{o} * k_{h} * k_{w} / g$</td>
</tr>
<tr>
<td>Depthwise Conv Layer</td>
<td>$c_{o} * k_{h} * k_{w}$</td>
</tr>
</tbody>
</table>
<p>其中，Grouped Conv指的是将输入在channel维度进行分组，然后分别进行卷积，最后concatenate。Depthwise Conv是分组个数 $g$ 等于输入channel数的情况。</p>
<p>除了这些weight外，还有bias以及norm相关的参数。
例如，对于batchnorm层，我们需要两个参数$\gamma,  \beta$以及runing mean和running variance。
$$
y = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$</p>
<p>这都是针对infernece的情况，对于training，还需要考虑一些额外的参数，例如momentum，以及保存梯度等等。</p>
<h4 id="model-size">model size</h4>
<p>$$
Model Size = \# parameters * bitwidth
$$</p>
<p>举个例子，AlexNet有$61M$个参数，如果我们用32bit的float来表示，那么model size就是$61M * 4byte = 224MB$。但如果我们用8-bit来表示每个weight，那么model size就是$61M * 1byte = 61MB$。</p>
<p>这就是quantization的一个应用，通过减少bitwidth来减少model size。</p>
<h4 id="totalpeak-activations">total/peak #activations</h4>
<p>#activations 是模型在推理时在内存中需要存储的中间结果，这也可能成为内存瓶颈。如下图所示：
<img loading="lazy" src="/img/tinyml/pruning/mcunet-activations.png" alt="activation"  />
</p>
<p>该图展示了MCUNet(一个专门用在IoT设备上的模型)的参数量和activations数量对比resnet的减少。可以看出，参数的减少量十分显著，但是# activations不降反增，和param的占比是一个数量级的。所以我们若想在边缘设备上部署模型，需要考虑activations的数量。</p>
<h4 id="mac">MAC</h4>
<p><code>MAC</code>的含义是 <strong>Multiply-Accumulate operation</strong>。例如，一个gemm(genearl matrix multiplication)的$MACs = m * n * k$（对于m * k的矩阵和k * n的矩阵相乘）。深度学习中几乎90%的时间都在做gemm，一个conv2d操作也可以由im2col转换为gemm操作。</p>
<p>以下是常见的一些layer的MACs计算：</p>
<table>
<thead>
<tr>
<th>layer</th>
<th>MACs(batch size = 1)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear</td>
<td>$feature_{in} * feature_{out}$</td>
</tr>
<tr>
<td>Conv</td>
<td>$c_{i}  * k_{h} * k_{w} * h_{o} * w_{o} * c_{o}$</td>
</tr>
<tr>
<td>grouped conv</td>
<td>$c_{i}/g * k_{h} * k_{w} * h_{o} * w_{o}* c_{o} $</td>
</tr>
<tr>
<td>depthwise conv</td>
<td>$c_{o} * k_{h} * k_{w} * h_{o} * w_{o}$</td>
</tr>
</tbody>
</table>
<p>（简单解释一下conv2d的MACs：对于每个输出特征图上的每个像素点，都需要进行 $c_i \times k_h \times k_w $ 次乘法运算和累加操作（假设每个卷积核和输入通道的对应部分做完整个卷积运算）。因为输出特征图有 $c_o$ 个通道，每个通道有 $h_o \times w_o$ 个像素点，所以总共的MACs数就是上面公式中的乘积。）</p>
<h4 id="flop-and-flops">FLOP and FLOPs</h4>
<p>FLOP的意思是 <strong>Floating Point Operations</strong>，是指浮点运算的次数。FLOPs是指每秒的FLOP数量。
一般一个MAC对应两个FLOP，一乘一加。</p>
<h3 id="energy-consumption">Energy Consumption</h3>
<p>对于边缘设备，我们还需要考虑能耗的问题。
<img loading="lazy" src="/img/tinyml/pruning/memory-is-expensive.png" alt="energy-consumption"  />

图中可以看出，从内存中取数操作的能耗是运算的200倍以上。</p>
<h2 id="neural-network-pruning">neural network pruning</h2>
<p>那么，到底什么是 <em>模型剪枝（model pruning）</em> 呢？顾名思义，就是将模型中的一些参数去掉，或者更加细粒度的，我们可以把某些参数与参数之间的连接去掉。还有一个常用的概念是 <em>稀疏度（sparsity）</em> ，指的是剪枝后的参数占原有参数的比例。例如，如果我们剪去了90%的参数，那么sparsity就是0.1。</p>
<p><img loading="lazy" src="/img/tinyml/pruning/nnpruning.png" alt="pruning"  />
</p>
<blockquote>
<p><strong>注意：剪去某个参数并不意味着单纯将其设置为0。<strong>因为我们剪枝的目的是</strong>减少内存使用</strong>以及<strong>加速推理</strong>，若只是讲某个$W$矩阵中的一些值变成0，并不能达到减少内存的目的。我们需要将剪枝后的参数$W_{p}$用特定的方式存储（例如，稀疏矩阵）以及结合特定的优化后的运算才能减少内存使用并加速。当然，从正确性的角度来说，将某些参数设置为0可以得到一样的结果。所以我们可以在确定 <em>sparsity</em> 时先将一定的参数置为0，然后再通过fine-tuning来确定剪去这么多的参数是否会影响模型最终的效果。</p>
</blockquote>
<p>减去一定的参数后，肯定会对我们的模型效果造成影响。这时候我们需要进行fine-tuning调整参数分布，具体会在后文展开。</p>
<p>形式化的定义prune：
<img loading="lazy" src="/img/tinyml/pruning/prune-formalized.png" alt="prune-formalized"  />
</p>
<h3 id="pruning-granularity">pruning granularity</h3>
<p>我们有不同的剪枝粒度，from unstructured to structured。</p>
<p>对于一个FC layer，其实就是将一个2d matrix进行剪枝：
<img loading="lazy" src="/img/tinyml/pruning/fc-prune.png" alt="fc-prune"  />

如图，细粒度的、不规则的剪枝一般能有更高的sparsity，但是却难以加速（就像上文说的，如果只是将一些参数设置为0，不会有任何内存与速度上的收益）。而粗粒度的剪枝，就可以不需要改变原有的矩阵结构获得更少的内存使用以及加速，但一般sparsity会高一些（同等accuracy下）。</p>
<p>对于卷积层，我们有更多的剪枝粒度：
<img loading="lazy" src="/img/tinyml/pruning/conv-prune.png" alt="conv-prune"  />

也是从完全不规则，到有一定的pattern，到vector-level，kernel-level再到剪去一个通道。优缺点如同上面分析的一般。
对于最fine-grained的剪枝，许多模型都可以达到剪去90%以上而不影响精度。
<img loading="lazy" src="/img/tinyml/pruning/fine-grained-prune-sparsity.png" alt="fine-grained-prune-sparsity"  />
</p>
<p>对于有一定pattern的剪枝，就需要设计特定的存储方式或者运算方式来加速。
例如下图的[N:M]剪枝（每M个参数剪去N个）就可以通过只存非0权重，以及一个下标数组的方式减少内存占用。nvdia的Ampere系列GPU就在其Tensor core内融合了这种优化，达到了2倍的加速效果
<img loading="lazy" src="/img/tinyml/pruning/pattern-grain-prune.png" alt="pattern-grain-prune"  />
</p>
<p>对于conv的剪枝，我们还用的一个不需要硬件特定实现的方法就是直接进行<code>channel-level pruning</code>，也就是直接减去特定的通道。</p>
<ul>
<li>pros: 直接的加速，无需特定硬件</li>
<li>cons: 更少的模型压缩比
不同的卷积层的sparsity又该如何确定呢？是所有卷积层的sparsity都设置的相同还是各不相同？有什么样的标准呢？我们会在在pruning ratio这里会讲到。</li>
</ul>
<h3 id="pruning-criterion">pruning criterion</h3>
<p>如何确定剪去的权重呢？常见的有<code>magnitude-based pruning</code>，<code>scaling-based pruning</code>等</p>
<h4 id="magnitude-based-pruning">Magnitude-based pruning</h4>
<blockquote>
<p>Learning Structured Sparsity in Deep Neural Networks <a href="https://arxiv.org/pdf/1608.03665.pdf">Wen et al., NeurIPS 2016</a></p>
</blockquote>
<p>只得就是通过权重的重要程度来进行剪枝。直觉上看，对于一个激活函数$y = ReLU(10x_0 - 8x_1 + 0.1x_2 )$，肯定是$0.1$这个权重对其结果影响最小，可以将其剪去。
依此定义重要度$Importance = |W|$，直接按照数值的大小进行剪枝。当然，我们也可以将其拓展到任意范数：
$$
Importance = |W|<em>p = (\sum</em>{i} |W_i|^p)^{\frac{1}{p}}
$$
只不过最常用的还是l1范数，因为这引入的额外的计算量较小。</p>
<h4 id="scaling-based-pruning">Scaling-based pruning</h4>
<blockquote>
<p>Learning Efficient Convolutional Networks through Network Slimming <a href="https://arxiv.org/pdf/1708.06519.pdf">Liu et al., ICCV 2017</a></p>
</blockquote>
<p>scaling是针对卷积的一种剪枝策略，我们对卷积操作后的每个通道都用一个 <em>scaling factor</em> 进行缩放。这个scaling factor就和其他参数一样是一个需要训练的参数。
<img loading="lazy" src="/img/tinyml/pruning/scaling-based-pruning.png" alt="scaling-prune"  />

我们在以scaling factor的大小代表这一channel的重要性，从而图中一样进行剪枝。事实上，我们并不需要额外的scaling factor，因为我们的batchnorm层有自带了一个scaling factor: $\gamma$ ($z = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$)。我们可以直接用这个$\gamma$来进行剪枝。</p>
<h4 id="second-order-based-pruning">Second-Order-based-pruning</h4>
<blockquote>
<p>Optimal Brain Damage <a href="https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf">LeCun et al., NeurIPS 1989</a></p>
</blockquote>
<p>前面两种剪枝标准都是启发式的，通过定义参数的重要性进行剪枝。我们还可以通过 <em>最小化剪枝前后的损失函数</em> 来进行剪枝。
$$
\delta L = L(\mathbf{x}; \mathbf{W}) - L(\mathbf{x}; \mathbf{W}<em>P = \mathbf{W} - \delta\mathbf{W})  = \sum</em>{i} g_{i}\delta w_{i} + \frac{1}{2}\sum_{i} h_{ii}\delta w_{i}^2 + \frac{1}{2}\sum_{i \neq j} h_{ij}\delta w_{i}\delta w_{j} + O(|\delta\mathbf{W}|^3) \ where\ \ g_i = \frac{\partial L}{\partial w_i}, h_{ij} = \frac{\partial^2 L}{\partial w_i \partial w_j}
$$
这是LeCun在1989年时候的工作，它假设：</p>
<ul>
<li>目标函数L是近似二次的，所以最后一项可忽略</li>
<li>这个神经网络的训练已经收敛了，这表明一阶导数（梯度）接近零，所以第一项 $(\sum_{i}g_{i}\delta w_{i})$ 也可忽略</li>
<li>删除每个参数引起的误差是独立的：这允许我们忽略混合二阶导数项 ，因为这些项描述的是参数之间的相互作用。</li>
</ul>
<p>因此，我们可以得到：
$$
\delta L_i = L(x; W) - L(x; W_p| w_i = 0) \approx \frac{1}{2}h_{ii} w_i^2
$$
也就是$importance_{w_i} = |\delta L| = \frac{1}{2}h_{ii}w_{i}^2$，这就是一个二阶导数的剪枝标准。但是，由于$h_{ii}$ 是Hessian matrix，计算量过大，所以一般不会使用。</p>
<h4 id="percentage-of-zero-based-pruning">percentage-of-Zero-Based pruning</h4>
<blockquote>
<p>Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures <a href="https://arxiv.org/pdf/1607.03250.pdf">Hu et al.,ArXiv 2017</a></p>
</blockquote>
<p>非常简单粗暴的对与Channel-level pruning的方法。由于Relu会产生0，我们直接定义每个通道的重要程度为其 <em>Average Percentage of Zeros(APoZ)</em>。选取最小APoZ的通道进行剪枝。</p>
<h4 id="regression-based-pruning">Regression-based pruning</h4>
<blockquote>
<p>Channel Pruning for Accelerating Very Deep Neural Networks <a href="https://arxiv.org/pdf/1707.06168.pdf">He et al., ICCV 2017</a></p>
</blockquote>
<p>regression-based pruning对于某一个layer进行操作，希望能够最小化重构误差。
令输入的feature map的channel数为$c$, 卷积核$W$的权重为 $n× c × k_h×k_w$, 卷积核每次卷积会在一个像素点上生成一个$N×n$的输出矩阵$Y$,其中$N$为batch_num，这里暂时不考虑bias项。要将$c$修剪为$c′$.同时最小化reconstruction error，这个优化问题是
$$
\begin{aligned}
&amp; \underset{\beta, W}{\text{arg min}}
&amp; &amp; \frac{1}{2N} | Y - \sum_{i=1}^{C} \beta_i X_i W_i^T |_F^2 \
&amp; \text{subject to}
&amp; &amp; | \beta |_0 \leq C'
\end{aligned}
$$</p>
<p>其中$X_{i}$是一个$N \times k_h k_w$的矩阵裁剪自输入$X$。求解这个问题是NP难的,这里首先将问题用l1范数松弛为</p>
<p>$$
\begin{aligned}
&amp; \underset{\beta, W}{\text{arg min}}
&amp; &amp; \frac{1}{2N} \left| Y - \sum_{i=1}^{C} \beta_i X_i W_i^T \right|_F^2 + \lambda \left| \beta \right|_1 \
&amp; \text{subject to}
&amp; &amp; \left| \beta \right|_0 \leq C&rsquo;, ; \forall i, \left| W_i \right|_F = 1
\end{aligned}
$$
同时限制$||Wi||F=1$，然后在以下两个步骤中迭代</p>
<ul>
<li>首先锁定$W$ ,求解$\beta$, 作为channel selection问题，这变成了零范数的LASSO regression。代码中可以知道作者是使用sklearn的Lasso regression函数做的</li>
<li>锁定$\beta$，问题变成了$\underset{W&rsquo;}{\text{arg min}} \left| Y - X&rsquo;(W&rsquo;)^T \right|_F^2
$本质上是一个线性回归。</li>
</ul>
<h3 id="pruning-ratio">Pruning Ratio</h3>
<p>对于不同的layer之间，怎么确定每个layer应该剪去多少的权重而不会过多的影响精度呢？</p>
<h4 id="sensitivity-of-each-layer">sensitivity of each layer</h4>
<p>一种直观的方法就是查看每个layer对权重的敏感度各是多少，具体操作如下：</p>
<ul>
<li>选定一个layer $L_i$
<ul>
<li>prune $li$ with ratio $r_i \in {0.1, 0.2, &hellip;, 0.9}$(or other strides)</li>
<li>对每个ratio观察精度下降$\Delta Acc_r^i$</li>
</ul>
</li>
<li>对所有layer重复上述操作</li>
</ul>
<p><img loading="lazy" src="/img/tinyml/pruning/prune-ratio.png" alt="prune-ratio"  />

图中是VGG-11在CIFAR-10数据集上的不同层的pruning ratio对精度的影响。选定一个threshold后就可以依据这个对不同layer选取不同的pruning ratio。</p>
<h4 id="automatic-pruning">automatic pruning</h4>
<p>上面的方法是一种手动的方法，并且没有考虑不同层之间的相互影响。
一个有意思的工作来自ECCV 2018, <a href="https://arxiv.org/pdf/1802.03494.pdf">AMC: AutoML for Model Compression and Acceleration on Mobile Devices</a>, 用了强化学习的方法来选取不同的pruning-ratio，效果很好。不过目前没看懂，后面有机会再补充。</p>
<h3 id="fine-tuningtraining">Fine-tuning/Training</h3>
<p>说了这么多pruing方法，具体的操作流程如下：
<img loading="lazy" src="/img/tinyml/pruning/iterative-prune.png" alt="iterative-prune"  />
</p>
<p>通常我们都会迭代式的再inference阶段进行pruning，例如，先采用50%的sparsity进行推理，得到精度误差后fine-tune一次，训练出新的weights分布，然后不断加大sparsity，不断重新fine-tune。如上图所示</p>
<h2 id="system--hardware-support-for-sparsity">System &amp; Hardware Support for Sparsity</h2>
<p>在pruning granularity的时候我们说到，若想要做到细粒度的剪枝，就需要特定的硬件支持。
韩老师自己的一篇论文[EIE](<a href="https://arxiv.org/pdf/1602.01528.pdfEIE">https://arxiv.org/pdf/1602.01528.pdfEIE</a>: Efficient Inference Engine on Compressed Deep Neural Network)就支持对任意粒度的剪枝进行加速。由于我不是做硬件的，暂时没有仔细研究实现。</p>
<p>英伟达的Amper系列GPU也支持对于structured sparsity的加速，例如[N:M]剪枝。
架构图如下：
<img loading="lazy" src="/img/tinyml/pruning/m-n-sparsity.png" alt="m-n-sparsity"  />
</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
