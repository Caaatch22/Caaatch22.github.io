<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>model-deployment on Mingjie&#39;s Home</title>
    <link>https://caaatch22.github.io/tags/model-deployment/</link>
    <description>Recent content in model-deployment on Mingjie&#39;s Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 27 Feb 2024 22:19:21 +0000</lastBuildDate><atom:link href="https://caaatch22.github.io/tags/model-deployment/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TinyMl —— quantization</title>
      <link>https://caaatch22.github.io/posts/tinyml-quantization/</link>
      <pubDate>Tue, 27 Feb 2024 22:19:21 +0000</pubDate>
      
      <guid>https://caaatch22.github.io/posts/tinyml-quantization/</guid>
      <description>Quantization 模型量化（quantization）指的是用更少的bit表示模型参数，从而减少模型的大小，加速推理过程的技术。
一种常见的量化方式是线性量化(linear quantization)，也叫仿射量化(affine quantization)。其实就是按比例将tensor（一般为fp32）放缩到 $2^{bitwidth}$ 的范围内，比如8bit等。我们很容易给出量化公式： $$ r = s(q - z) $$ 其中，r(real value)值得是量化前的值，q(quantized value)是量化后的值，s(scale)是放缩比例，z(zero point)相当于是一个偏移量。
如何求出$s$和$z$呢？一种简单且常见的方式是通过最大最小值来估计，即：
$$ s = \frac{r_{max} - r_{min}}{q_{max} - q_{min}} $$ $r_{max}$就是这个tensor的最大值，$r_{min}$是最小值，$q_{max}$和$q_{min}$是我们指定的量化后的最大最小值。如下图所示： 有了scale, 容易得到 $z = q_{min} - \frac{r_{min}}{s}$。在实际操作中，z一般会被round到最近的整数$z = round(q_{min} - \frac{r_{min}}{s})$（有很多不同的round规则，这个有具体实现决定）。
得到量化方程： $$ q = clip(round(\frac{r}{s}) + z, q_{min}, q_{max}) $$
代码示意如下：（实际会用pytorch已有的quantize api或者其他推理框架）
def get_quantized_range(bitwidth): quantized_max = (1 &amp;lt;&amp;lt; (bitwidth - 1)) - 1 quantized_min = -(1 &amp;lt;&amp;lt; (bitwidth - 1)) return quantized_min, quantized_max def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.</description>
      <content:encoded><![CDATA[<h2 id="quantization">Quantization</h2>
<p>模型量化（quantization）指的是用更少的bit表示模型参数，从而减少模型的大小，加速推理过程的技术。</p>
<p>一种常见的量化方式是线性量化(linear quantization)，也叫仿射量化(affine quantization)。其实就是按比例将tensor（一般为fp32）放缩到 $2^{bitwidth}$ 的范围内，比如8bit等。我们很容易给出量化公式：
$$
r = s(q - z)
$$
其中，r(real value)值得是量化前的值，q(quantized value)是量化后的值，s(scale)是放缩比例，z(zero point)相当于是一个偏移量。</p>
<p>如何求出$s$和$z$呢？一种简单且常见的方式是通过最大最小值来估计，即：</p>
<p>$$
s = \frac{r_{max} - r_{min}}{q_{max} - q_{min}}
$$
$r_{max}$就是这个tensor的最大值，$r_{min}$是最小值，$q_{max}$和$q_{min}$是我们指定的量化后的最大最小值。如下图所示：
<img loading="lazy" src="/img/tinyml/quantization/quantization.png" alt="image"  />
</p>
<p>有了scale, 容易得到 $z = q_{min} - \frac{r_{min}}{s}$。在实际操作中，z一般会被round到最近的整数$z = round(q_{min} - \frac{r_{min}}{s})$（有很多不同的round规则，这个有具体实现决定）。</p>
<p>得到量化方程：
$$
q = clip(round(\frac{r}{s}) + z, q_{min}, q_{max})
$$</p>
<p>代码示意如下：（实际会用pytorch已有的quantize api或者其他推理框架）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_quantized_range</span>(bitwidth):
</span></span><span style="display:flex;"><span>    quantized_max <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> (bitwidth <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    quantized_min <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>(<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> (bitwidth <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quantized_min, quantized_max
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linear_quantize</span>(fp_tensor, bitwidth, scale, zero_point, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>int8) <span style="color:#f92672">-&gt;</span> torch<span style="color:#f92672">.</span>Tensor:
</span></span><span style="display:flex;"><span>    rounded_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>round(fp_tensor <span style="color:#f92672">/</span> scale)<span style="color:#f92672">.</span>to(dtype)
</span></span><span style="display:flex;"><span>    shifted_tensor <span style="color:#f92672">=</span> rounded_tensor <span style="color:#f92672">+</span> zero_point
</span></span><span style="display:flex;"><span>    quantized_min, quantized_max <span style="color:#f92672">=</span> get_quantized_range(bitwidth)
</span></span><span style="display:flex;"><span>    quantized_tensor <span style="color:#f92672">=</span> shifted_tensor<span style="color:#f92672">.</span>clamp_(quantized_min, quantized_max)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quantized_tensor
</span></span></code></pre></div><p>上述过程被称为非对称量化(asymmetric quantization)。下图是一个训练好的vgg的模型参数分布：</p>
<p><img loading="lazy" src="/img/tinyml/quantization/weight-distribution.png" alt="weight distribution"  />
</p>
<p>我们发现，几乎所有卷积层的参数都关于0对称，基于这个观察，在量化的过程中，我们常用对称量化(symmetric quantization)。即，我们将zero point设置为0，并让q_{min} = -q_{max}。这样，我们就可以简化量化公式为：
$$
q = clip(round(\frac{r}{s}), -q_{max}, q_{max})
$$</p>
<p>进一步的，当我们进行推理的过程中，设$Y = WX + b$</p>
<h2 id="post-training-quantizationptq">Post-training quantization(PTQ)</h2>
<p>$$
q_{output} = (linear(q_{input}, q_{weight}) + Q_{bias}) * (s_{input} * s_{weight} / s_{output})
$$</p>
<h2 id="qauntization-aware-trainingqat">Qauntization-aware training(QAT)</h2>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
